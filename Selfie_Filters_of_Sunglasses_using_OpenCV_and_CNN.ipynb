{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Selfie Filters of Sunglasses using OpenCV and CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "toTcZpabBoc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fc6aad4-d204-4084-a74c-eb80565b837e"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n",
        "def get_my_CNN_model_architecture():\n",
        "    '''\n",
        "    The network should accept a 96x96 grayscale image as input, and it should output a vector with 30 entries,\n",
        "    corresponding to the predicted (horizontal and vertical) locations of 15 facial keypoints.\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(32, (5, 5), input_shape=(96,96,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Convolution2D(30, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(30))\n",
        "\n",
        "    return model;\n",
        "\n",
        "def compile_my_CNN_model(model, optimizer, loss, metrics):\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "def train_my_CNN_model(model, X_train, y_train):\n",
        "    return model.fit(X_train, y_train, epochs=100, batch_size=200, verbose=1, validation_split=0.2)\n",
        "\n",
        "def save_my_CNN_model(model, fileName):\n",
        "    model.save(fileName + '.h5')\n",
        "\n",
        "def load_my_CNN_model(fileName):\n",
        "    return load_model(fileName + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExy7vAlFyFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0cc6fd3-5f43-4304-fd96-0842ac95ccf5"
      },
      "source": [
        "from utils import load_data\n",
        "# from my_CNN_model import *\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Load training set\n",
        "X_train, y_train = load_data()\n",
        "\n",
        "# NOTE: Please check the load_data() method in utils.py to see how the data is preprocessed (normalizations and stuff)\n",
        "\n",
        "\n",
        "# Setting the CNN architecture\n",
        "my_model = get_my_CNN_model_architecture()\n",
        "\n",
        "# Compiling the CNN model with an appropriate optimizer and loss and metrics\n",
        "compile_my_CNN_model(my_model, optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "hist = train_my_CNN_model(my_model, X_train, y_train)\n",
        "\n",
        "# train_my_CNN_model returns a History object. History.history attribute is a record of training loss values and metrics\n",
        "# values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
        "\n",
        "# Saving the model\n",
        "save_my_CNN_model(my_model, 'my_model')\n",
        "\n",
        "'''\n",
        "# You can skip all the steps above (from 'Setting the CNN architecture') after running the script for the first time.\n",
        "# Just load the recent model using load_my_CNN_model and use it to predict keypoints on any face data\n",
        "my_model = load_my_CNN_model('my_model')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1712 samples, validate on 428 samples\n",
            "Epoch 1/100\n",
            "1712/1712 [==============================] - 32s 19ms/step - loss: 0.1064 - accuracy: 0.0117 - val_loss: 0.0671 - val_accuracy: 0.0187\n",
            "Epoch 2/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0361 - accuracy: 0.0129 - val_loss: 0.0236 - val_accuracy: 0.0327\n",
            "Epoch 3/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0135 - accuracy: 0.4971 - val_loss: 0.0148 - val_accuracy: 0.2523\n",
            "Epoch 4/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0076 - accuracy: 0.5216 - val_loss: 0.0144 - val_accuracy: 0.6963\n",
            "Epoch 5/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0061 - accuracy: 0.7074 - val_loss: 0.0085 - val_accuracy: 0.6963\n",
            "Epoch 6/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0061 - accuracy: 0.7074 - val_loss: 0.0097 - val_accuracy: 0.6963\n",
            "Epoch 7/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0054 - accuracy: 0.7074 - val_loss: 0.0124 - val_accuracy: 0.6963\n",
            "Epoch 8/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0050 - accuracy: 0.7074 - val_loss: 0.0123 - val_accuracy: 0.6963\n",
            "Epoch 9/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0049 - accuracy: 0.7074 - val_loss: 0.0116 - val_accuracy: 0.6963\n",
            "Epoch 10/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0049 - accuracy: 0.7074 - val_loss: 0.0130 - val_accuracy: 0.6963\n",
            "Epoch 11/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0048 - accuracy: 0.7074 - val_loss: 0.0119 - val_accuracy: 0.6963\n",
            "Epoch 12/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0047 - accuracy: 0.7074 - val_loss: 0.0115 - val_accuracy: 0.6963\n",
            "Epoch 13/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0047 - accuracy: 0.7074 - val_loss: 0.0106 - val_accuracy: 0.6963\n",
            "Epoch 14/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0046 - accuracy: 0.7074 - val_loss: 0.0116 - val_accuracy: 0.6963\n",
            "Epoch 15/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0047 - accuracy: 0.7074 - val_loss: 0.0112 - val_accuracy: 0.6963\n",
            "Epoch 16/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0047 - accuracy: 0.7074 - val_loss: 0.0113 - val_accuracy: 0.6963\n",
            "Epoch 17/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0047 - accuracy: 0.7074 - val_loss: 0.0108 - val_accuracy: 0.6963\n",
            "Epoch 18/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0046 - accuracy: 0.7074 - val_loss: 0.0100 - val_accuracy: 0.6963\n",
            "Epoch 19/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0045 - accuracy: 0.7074 - val_loss: 0.0085 - val_accuracy: 0.6963\n",
            "Epoch 20/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0045 - accuracy: 0.7074 - val_loss: 0.0071 - val_accuracy: 0.6963\n",
            "Epoch 21/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0046 - accuracy: 0.7074 - val_loss: 0.0080 - val_accuracy: 0.6963\n",
            "Epoch 22/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0044 - accuracy: 0.7074 - val_loss: 0.0076 - val_accuracy: 0.6963\n",
            "Epoch 23/100\n",
            "1712/1712 [==============================] - 31s 18ms/step - loss: 0.0044 - accuracy: 0.7074 - val_loss: 0.0087 - val_accuracy: 0.6963\n",
            "Epoch 24/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0044 - accuracy: 0.7074 - val_loss: 0.0068 - val_accuracy: 0.6963\n",
            "Epoch 25/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0045 - accuracy: 0.7074 - val_loss: 0.0071 - val_accuracy: 0.6963\n",
            "Epoch 26/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0044 - accuracy: 0.7074 - val_loss: 0.0062 - val_accuracy: 0.6963\n",
            "Epoch 27/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0043 - accuracy: 0.7074 - val_loss: 0.0059 - val_accuracy: 0.6963\n",
            "Epoch 28/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0043 - accuracy: 0.7074 - val_loss: 0.0061 - val_accuracy: 0.6963\n",
            "Epoch 29/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0043 - accuracy: 0.7074 - val_loss: 0.0057 - val_accuracy: 0.6963\n",
            "Epoch 30/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0042 - accuracy: 0.7074 - val_loss: 0.0061 - val_accuracy: 0.6963\n",
            "Epoch 31/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0042 - accuracy: 0.7074 - val_loss: 0.0055 - val_accuracy: 0.6963\n",
            "Epoch 32/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0042 - accuracy: 0.7074 - val_loss: 0.0056 - val_accuracy: 0.6963\n",
            "Epoch 33/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0041 - accuracy: 0.7074 - val_loss: 0.0052 - val_accuracy: 0.6963\n",
            "Epoch 34/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0041 - accuracy: 0.7074 - val_loss: 0.0060 - val_accuracy: 0.6963\n",
            "Epoch 35/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0040 - accuracy: 0.7074 - val_loss: 0.0060 - val_accuracy: 0.6963\n",
            "Epoch 36/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0040 - accuracy: 0.7074 - val_loss: 0.0053 - val_accuracy: 0.6963\n",
            "Epoch 37/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0039 - accuracy: 0.7074 - val_loss: 0.0051 - val_accuracy: 0.6963\n",
            "Epoch 38/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0038 - accuracy: 0.7074 - val_loss: 0.0049 - val_accuracy: 0.6963\n",
            "Epoch 39/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0037 - accuracy: 0.7074 - val_loss: 0.0047 - val_accuracy: 0.6963\n",
            "Epoch 40/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0037 - accuracy: 0.7074 - val_loss: 0.0047 - val_accuracy: 0.6963\n",
            "Epoch 41/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0035 - accuracy: 0.7068 - val_loss: 0.0037 - val_accuracy: 0.6986\n",
            "Epoch 42/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0034 - accuracy: 0.7068 - val_loss: 0.0040 - val_accuracy: 0.6986\n",
            "Epoch 43/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0033 - accuracy: 0.7103 - val_loss: 0.0048 - val_accuracy: 0.7033\n",
            "Epoch 44/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0033 - accuracy: 0.7120 - val_loss: 0.0042 - val_accuracy: 0.7079\n",
            "Epoch 45/100\n",
            "1712/1712 [==============================] - 31s 18ms/step - loss: 0.0032 - accuracy: 0.7150 - val_loss: 0.0042 - val_accuracy: 0.7079\n",
            "Epoch 46/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0030 - accuracy: 0.7161 - val_loss: 0.0034 - val_accuracy: 0.7126\n",
            "Epoch 47/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0030 - accuracy: 0.6963 - val_loss: 0.0033 - val_accuracy: 0.7126\n",
            "Epoch 48/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0029 - accuracy: 0.7126 - val_loss: 0.0031 - val_accuracy: 0.7126\n",
            "Epoch 49/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0028 - accuracy: 0.7097 - val_loss: 0.0033 - val_accuracy: 0.7150\n",
            "Epoch 50/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0028 - accuracy: 0.7044 - val_loss: 0.0032 - val_accuracy: 0.7126\n",
            "Epoch 51/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0027 - accuracy: 0.7109 - val_loss: 0.0027 - val_accuracy: 0.7173\n",
            "Epoch 52/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0026 - accuracy: 0.7120 - val_loss: 0.0030 - val_accuracy: 0.7150\n",
            "Epoch 53/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0026 - accuracy: 0.7114 - val_loss: 0.0030 - val_accuracy: 0.7150\n",
            "Epoch 54/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0025 - accuracy: 0.7161 - val_loss: 0.0029 - val_accuracy: 0.7150\n",
            "Epoch 55/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0025 - accuracy: 0.7079 - val_loss: 0.0030 - val_accuracy: 0.7150\n",
            "Epoch 56/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0024 - accuracy: 0.7132 - val_loss: 0.0026 - val_accuracy: 0.7243\n",
            "Epoch 57/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0023 - accuracy: 0.7126 - val_loss: 0.0027 - val_accuracy: 0.7220\n",
            "Epoch 58/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0024 - accuracy: 0.7173 - val_loss: 0.0027 - val_accuracy: 0.7220\n",
            "Epoch 59/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0023 - accuracy: 0.7150 - val_loss: 0.0027 - val_accuracy: 0.7150\n",
            "Epoch 60/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0022 - accuracy: 0.7179 - val_loss: 0.0023 - val_accuracy: 0.7266\n",
            "Epoch 61/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0022 - accuracy: 0.7144 - val_loss: 0.0023 - val_accuracy: 0.7196\n",
            "Epoch 62/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0022 - accuracy: 0.7126 - val_loss: 0.0022 - val_accuracy: 0.7079\n",
            "Epoch 63/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0021 - accuracy: 0.7144 - val_loss: 0.0022 - val_accuracy: 0.7079\n",
            "Epoch 64/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0021 - accuracy: 0.7272 - val_loss: 0.0022 - val_accuracy: 0.7103\n",
            "Epoch 65/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0021 - accuracy: 0.7138 - val_loss: 0.0020 - val_accuracy: 0.7173\n",
            "Epoch 66/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0021 - accuracy: 0.7214 - val_loss: 0.0020 - val_accuracy: 0.7079\n",
            "Epoch 67/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0020 - accuracy: 0.7103 - val_loss: 0.0020 - val_accuracy: 0.7173\n",
            "Epoch 68/100\n",
            "1712/1712 [==============================] - 32s 19ms/step - loss: 0.0020 - accuracy: 0.7155 - val_loss: 0.0020 - val_accuracy: 0.7266\n",
            "Epoch 69/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0020 - accuracy: 0.7278 - val_loss: 0.0019 - val_accuracy: 0.7150\n",
            "Epoch 70/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0019 - accuracy: 0.7202 - val_loss: 0.0019 - val_accuracy: 0.7126\n",
            "Epoch 71/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0019 - accuracy: 0.7196 - val_loss: 0.0020 - val_accuracy: 0.7173\n",
            "Epoch 72/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0019 - accuracy: 0.7167 - val_loss: 0.0018 - val_accuracy: 0.7103\n",
            "Epoch 73/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0019 - accuracy: 0.7208 - val_loss: 0.0019 - val_accuracy: 0.7150\n",
            "Epoch 74/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0018 - accuracy: 0.7173 - val_loss: 0.0017 - val_accuracy: 0.7243\n",
            "Epoch 75/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0018 - accuracy: 0.7150 - val_loss: 0.0017 - val_accuracy: 0.7126\n",
            "Epoch 76/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0018 - accuracy: 0.7296 - val_loss: 0.0017 - val_accuracy: 0.7103\n",
            "Epoch 77/100\n",
            "1712/1712 [==============================] - 28s 16ms/step - loss: 0.0018 - accuracy: 0.7307 - val_loss: 0.0016 - val_accuracy: 0.7150\n",
            "Epoch 78/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0018 - accuracy: 0.7331 - val_loss: 0.0016 - val_accuracy: 0.7150\n",
            "Epoch 79/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0017 - accuracy: 0.7266 - val_loss: 0.0016 - val_accuracy: 0.7220\n",
            "Epoch 80/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0017 - accuracy: 0.7196 - val_loss: 0.0016 - val_accuracy: 0.7220\n",
            "Epoch 81/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0017 - accuracy: 0.7231 - val_loss: 0.0017 - val_accuracy: 0.7173\n",
            "Epoch 82/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7348 - val_loss: 0.0017 - val_accuracy: 0.7196\n",
            "Epoch 83/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7360 - val_loss: 0.0017 - val_accuracy: 0.7243\n",
            "Epoch 84/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7307 - val_loss: 0.0017 - val_accuracy: 0.7173\n",
            "Epoch 85/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7407 - val_loss: 0.0017 - val_accuracy: 0.7126\n",
            "Epoch 86/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7377 - val_loss: 0.0016 - val_accuracy: 0.7173\n",
            "Epoch 87/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0016 - accuracy: 0.7354 - val_loss: 0.0016 - val_accuracy: 0.7103\n",
            "Epoch 88/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0015 - accuracy: 0.7225 - val_loss: 0.0017 - val_accuracy: 0.7266\n",
            "Epoch 89/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0015 - accuracy: 0.7301 - val_loss: 0.0015 - val_accuracy: 0.7243\n",
            "Epoch 90/100\n",
            "1712/1712 [==============================] - 31s 18ms/step - loss: 0.0015 - accuracy: 0.7465 - val_loss: 0.0015 - val_accuracy: 0.7126\n",
            "Epoch 91/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0015 - accuracy: 0.7342 - val_loss: 0.0015 - val_accuracy: 0.7150\n",
            "Epoch 92/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0015 - accuracy: 0.7366 - val_loss: 0.0014 - val_accuracy: 0.7220\n",
            "Epoch 93/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7360 - val_loss: 0.0014 - val_accuracy: 0.7220\n",
            "Epoch 94/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7395 - val_loss: 0.0015 - val_accuracy: 0.7243\n",
            "Epoch 95/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7418 - val_loss: 0.0014 - val_accuracy: 0.7196\n",
            "Epoch 96/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7518 - val_loss: 0.0014 - val_accuracy: 0.7196\n",
            "Epoch 97/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7447 - val_loss: 0.0014 - val_accuracy: 0.7150\n",
            "Epoch 98/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7395 - val_loss: 0.0014 - val_accuracy: 0.7313\n",
            "Epoch 99/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7389 - val_loss: 0.0014 - val_accuracy: 0.7313\n",
            "Epoch 100/100\n",
            "1712/1712 [==============================] - 27s 16ms/step - loss: 0.0014 - accuracy: 0.7342 - val_loss: 0.0014 - val_accuracy: 0.7266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# You can skip all the steps above (from 'Setting the CNN architecture') after running the script for the first time.\\n# Just load the recent model using load_my_CNN_model and use it to predict keypoints on any face data\\nmy_model = load_my_CNN_model('my_model')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEieF6YlJkHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from IPython.display import display, Javascript\n",
        "# from google.colab.output import eval_js\n",
        "# from base64 import b64decode\n",
        "\n",
        "# def take_photo(filename='photo.jpg', quality=0.8):\n",
        "#   js = Javascript('''\n",
        "#     async function takePhoto(quality) {\n",
        "#       const div = document.createElement('div');\n",
        "#       const capture = document.createElement('button');\n",
        "#       capture.textContent = 'Capture';\n",
        "#       div.appendChild(capture);\n",
        "\n",
        "#       const video = document.createElement('video');\n",
        "#       video.style.display = 'block';\n",
        "#       const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "#       document.body.appendChild(div);\n",
        "#       div.appendChild(video);\n",
        "#       video.srcObject = stream;\n",
        "#       await video.play();\n",
        "\n",
        "#       // Resize the output to fit the video element.\n",
        "#       google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "#       // Wait for Capture to be clicked.\n",
        "#       await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "#       const canvas = document.createElement('canvas');\n",
        "#       canvas.width = video.videoWidth;\n",
        "#       canvas.height = video.videoHeight;\n",
        "#       canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "#       stream.getVideoTracks()[0].stop();\n",
        "#       div.remove();\n",
        "#       return canvas.toDataURL('image/jpeg', quality);\n",
        "#     }\n",
        "#     ''')\n",
        "#   display(js)\n",
        "#   data = eval_js('takePhoto({})'.format(quality))\n",
        "#   binary = b64decode(data.split(',')[1])\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(binary)\n",
        "#   return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK4sNbQJJkFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VSiAywvGImq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from my_CNN_model import *\n",
        "import cv2\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the model built in the previous step\n",
        "my_model = load_my_CNN_model('my_model')\n",
        "\n",
        "# Face cascade to detect faces\n",
        "face_cascade = cv2.CascadeClassifier('cascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Define the upper and lower boundaries for a color to be considered \"Blue\"\n",
        "blueLower = np.array([100, 60, 60])\n",
        "blueUpper = np.array([140, 255, 255])\n",
        "\n",
        "# Define a 5x5 kernel for erosion and dilation\n",
        "kernel = np.ones((5, 5), np.uint8)\n",
        "\n",
        "# Define filters\n",
        "filters = ['images/sunglasses.png', 'images/sunglasses_2.png']\n",
        "filterIndex = 0\n",
        "\n",
        "# Load the video\n",
        "camera = cv2.VideoCapture(0)\n",
        "#for google colab\n",
        "# from IPython.display import Image\n",
        "# try:\n",
        "#   camera = take_photo()\n",
        "#   # print('Saved to {}'.format(filename))\n",
        "  \n",
        "#   # Show the image which was just taken.\n",
        "#   # display(Image(filename))\n",
        "# except Exception as err:\n",
        "#   # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "#   # grant the page permission to access it.\n",
        "#   print(str(err))\n",
        "\n",
        "# Keep looping\n",
        "\n",
        "while True:\n",
        "    # Grab the current paintWindow\n",
        "    \n",
        "    (grabbed, frame) = camera.read()\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    frame2 = np.copy(frame)\n",
        "    # print(frame.shape)\n",
        "    if frame is not None:\n",
        "      hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      # Add the 'Next Filter' button to the frame\n",
        "      frame = cv2.rectangle(frame, (500,10), (620,65), (235,50,50), -1)\n",
        "    \n",
        "    cv2.putText(frame, \"NEXT FILTER\", (512, 37), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
        "\n",
        "    # Determine which pixels fall within the blue boundaries and then blur the binary image\n",
        "    blueMask = cv2.inRange(hsv, blueLower, blueUpper)\n",
        "    blueMask = cv2.erode(blueMask, kernel, iterations=2)\n",
        "    blueMask = cv2.morphologyEx(blueMask, cv2.MORPH_OPEN, kernel)\n",
        "    blueMask = cv2.dilate(blueMask, kernel, iterations=1)\n",
        "\n",
        "    # Find contours (bottle cap in my case) in the image\n",
        "    (cnts, _) = cv2.findContours(blueMask.copy(), cv2.RETR_EXTERNAL,\n",
        "    \tcv2.CHAIN_APPROX_SIMPLE)\n",
        "    center = None\n",
        "\n",
        "    # Check to see if any contours were found\n",
        "    if len(cnts) > 0:\n",
        "    \t# Sort the contours and find the largest one -- we\n",
        "    \t# will assume this contour correspondes to the area of the bottle cap\n",
        "        cnt = sorted(cnts, key = cv2.contourArea, reverse = True)[0]\n",
        "        # Get the radius of the enclosing circle around the found contour\n",
        "        ((x, y), radius) = cv2.minEnclosingCircle(cnt)\n",
        "        # Draw the circle around the contour\n",
        "        cv2.circle(frame, (int(x), int(y)), int(radius), (0, 255, 255), 2)\n",
        "        # Get the moments to calculate the center of the contour (in this case Circle)\n",
        "        M = cv2.moments(cnt)\n",
        "        center = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
        "\n",
        "        if center[1] <= 65:\n",
        "            if 500 <= center[0] <= 620: # Next Filter\n",
        "                filterIndex += 1\n",
        "                filterIndex %= 6\n",
        "                continue\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        # Grab the face\n",
        "        gray_face = gray[y:y+h, x:x+w]\n",
        "        color_face = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Normalize to match the input format of the model - Range of pixel to [0, 1]\n",
        "        gray_normalized = gray_face / 255\n",
        "\n",
        "        # Resize it to 96x96 to match the input format of the model\n",
        "        original_shape = gray_face.shape # A Copy for future reference\n",
        "        face_resized = cv2.resize(gray_normalized, (96, 96), interpolation = cv2.INTER_AREA)\n",
        "        face_resized_copy = face_resized.copy()\n",
        "        face_resized = face_resized.reshape(1, 96, 96, 1)\n",
        "\n",
        "        # Predicting the keypoints using the model\n",
        "        keypoints = my_model.predict(face_resized)\n",
        "\n",
        "        # De-Normalize the keypoints values\n",
        "        keypoints = keypoints * 48 + 48\n",
        "\n",
        "        # Map the Keypoints back to the original image\n",
        "        face_resized_color = cv2.resize(color_face, (96, 96), interpolation = cv2.INTER_AREA)\n",
        "        face_resized_color2 = np.copy(face_resized_color)\n",
        "\n",
        "        # Pair them together\n",
        "        points = []\n",
        "        for i, co in enumerate(keypoints[0][0::2]):\n",
        "            points.append((co, keypoints[0][1::2][i]))\n",
        "\n",
        "        # Add FILTER to the frame\n",
        "        sunglasses = cv2.imread(filters[filterIndex], cv2.IMREAD_UNCHANGED)\n",
        "        sunglass_width = int((points[7][0]-points[9][0])*1.1)\n",
        "        sunglass_height = int((points[10][1]-points[8][1])/1.1)\n",
        "        sunglass_resized = cv2.resize(sunglasses, (sunglass_width, sunglass_height), interpolation = cv2.INTER_CUBIC)\n",
        "        transparent_region = sunglass_resized[:,:,:3] != 0\n",
        "        face_resized_color[int(points[9][1]):int(points[9][1])+sunglass_height, int(points[9][0]):int(points[9][0])+sunglass_width,:][transparent_region] = sunglass_resized[:,:,:3][transparent_region]\n",
        "\n",
        "        # Resize the face_resized_color image back to its original shape\n",
        "        frame[y:y+h, x:x+w] = cv2.resize(face_resized_color, original_shape, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "        # Add KEYPOINTS to the frame2\n",
        "        for keypoint in points:\n",
        "            cv2.circle(face_resized_color2, keypoint, 1, (0,255,0), 1)\n",
        "\n",
        "        frame2[y:y+h, x:x+w] = cv2.resize(face_resized_color2, original_shape, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "        # Show the frame and the frame2\n",
        "        cv2.imshow(\"Selfie Filters\", frame)\n",
        "        cv2.imshow(\"Facial Keypoints\", frame2)\n",
        "\n",
        "        #if you're using google colab then use the following lines\n",
        "        # cv2_imshow(\"Selfie Filters\", frame)\n",
        "        # cv2_imshow(\"Facial Keypoints\", frame2)\n",
        "\n",
        "    # If the 'q' key is pressed, stop the loop\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# Cleanup the camera and close any open windows\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv_HvV46Jp-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from IPython.display import Image\n",
        "# try:\n",
        "#   filename = take_photo()\n",
        "#   print('Saved to {}'.format(filename))\n",
        "  \n",
        "#   # Show the image which was just taken.\n",
        "#   display(Image(filename))\n",
        "# except Exception as err:\n",
        "#   # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "#   # grant the page permission to access it.\n",
        "#   print(str(err))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C6TwHzgKRQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}